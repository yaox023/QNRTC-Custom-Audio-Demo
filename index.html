<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8"/>
    <title>One on One Sample</title>
    <!-- sdk源文件git地址：https://github.com/pili-engineering/QNRTC-Web/tree/master/Release/qnweb-rtc.js -->
    <!-- 该地址仅供测试，不保证稳定，正式环境切勿使用 -->
    <script src="https://docs.qnsdk.com/qnweb-rtc.js"></script>
    <style>
        select {
            width: 300px;
        }

        section {
            margin-bottom: 20px;
        }

        #localtracks, #remotetracks {
            width: 320px;
            height: 240px;
            background: #000;
        }
    </style>
</head>
<body>
<label>请输入 RoomToken 加入房间开始连麦</label>
<input id="roomtoken" type="text"/>
<p class="tips">如果您不知道如何生成 RoomToken，查看<a href="https://developer.qiniu.io/rtc/8802/pd-overview"
                                          target="_black">这里的接入流程</a></p>
<button onclick="joinRoom()">加入房间</button>
<p>本地视频</p>
<div id="localtracks"></div>
<p>远端视频</p>
<div id="remotetracks"></div>
</body>
</html>
<script>
    // 确认引入成功
    console.log("current version", QNRTC.VERSION);

    async function joinRoom() {
        const client = QNRTC.createClient();
        autoSubscribe(client);
        const roomTokenInput = document.getElementById("roomtoken");
        const roomToken = roomTokenInput.value;
        await client.join(roomToken);
        console.log("joinRoom success!");
    }

    let targetTrack;
    let processedTrack;

    function autoSubscribe(client) {
        client.on("user-published", async (userId, tracks) => {
            console.log("user-published!", userId, tracks);

            const remoteTracks = await client.subscribe(tracks);
            targetTrack = remoteTracks.audioTracks[0]
            console.log("target audio track: ", targetTrack)

            const audioCtx = new AudioContext()

            const mediaStream = new MediaStream();

            const mediaStreamTrack = targetTrack.getMediaStreamTrack()
            mediaStream.addTrack(mediaStreamTrack)

            // const stream = await navigator.mediaDevices.getUserMedia({audio:true})
            // stream.getAudioTracks().forEach(track => mediaStream.addTrack(track));

            const mutedAudio = new Audio();
            mutedAudio.muted = true;
            mutedAudio.srcObject = mediaStream;
            mutedAudio.play();

            const scriptNode = audioCtx.createScriptProcessor(16384)

            // 注意在下面这个回调函数里面做数据处理
            scriptNode.onaudioprocess = e => {
                // 注意这里两个数据通道
                // 逻辑是，从 input 里面拿数据，处理后数据设置到 output 里面去
                let inputBuffer = e.inputBuffer;
                let outputBuffer = e.outputBuffer;
                // console.log(e.inputBuffer.getChannelData(0))

                // 这里的逻辑是不动，直接把 input 拷贝到 output
                // 如果要做逻辑的话，就是 input => data => processed data => output
                // 这里实现逻辑比较 tricky，可能要做缓存队列、注意顺序等等，效果要看实际测试结果
                for (let channel = 0; channel < outputBuffer.numberOfChannels; channel++) {
                    let inputData = inputBuffer.getChannelData(channel);
                    let outputData = outputBuffer.getChannelData(channel);

                    for (let sample = 0; sample < inputBuffer.length; sample++) {
                        outputData[sample] = inputData[sample];
                    }
                }
            }

            const source = audioCtx.createMediaStreamSource(mediaStream)
            const dest = audioCtx.createMediaStreamDestination()

            source.connect(scriptNode)
            scriptNode.connect(dest)
            // play(dest.stream)
            // play(mediaStream)

            processedTrack = QNRTC.createCustomAudioTrack({mediaStreamTrack: dest.stream.getAudioTracks()[0]})
            await client.publish(processedTrack)
        });
    }

    function play(stream) {
        const audio = new Audio()
        audio.srcObject = stream
        audio.play()
    }



</script>